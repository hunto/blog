---
layout: post
cover: 'http://bbs.dian.org.cn/assets/uploads/files/1530788239704-c2592089-9346-4638-8f0d-6a0059153cdc-image.png'
title: 'CS224n笔记 - lecture2 - Word Vectors'
subtitle: 'Word Vectors'
date: 2018-07-05
categories: CS224n
tags: CS224n 机器学习 深度学习 NLP
---

所有课件及Assignments可见我的[Github:hunto/CS224n](https://github.com/hunto/CS224n)

# Lecture2 - 词向量

---
## 1. 如何表示一个词语？
**一个新词语的意思是通过已知词语来表示的**

![0_1530785759492_0f4b67cd-2371-471f-8de3-91f4bd6b220d-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530785760265-0f4b67cd-2371-471f-8de3-91f4bd6b220d-image.png) 

WordNet这样解释词语的问题
* 缺少对词语见细微差别的表示
* 缺少词语新的意思 -- 词的意思不是一成不变的，但是保证对词语的解释永远是最新的是**不可能**的
* 主观的
* 需要人为构建和适配
* 难以计算细微的词语相似度

---
因此，我们可以将词使用离散的数学标记来表示。
词语可以使用one-hot向量 [*one-hot意思为向量中有1个1，其余为0*] 的方式来表示，例如：
```
辣鸡 = [1, 0, 0, 0]
优秀 = [0, 1, 0, 0]
```
这样得到的向量的维度 = 词库中的词语数

但是，这样的表示方法也存在很多问题：
例如：
```
辣鸡 = [1, 0, 0, 0]
垃圾 = [0, 0, 1, 0]
```
但在具体语境中，`辣鸡zzy`和`垃圾zzy`是相近或者说等价的，可他们放到one-hot中的表示却是不同的两个向量，因此，这样的表示方法无法表示出词语间的相似度（one-hot向量间的距离与他们的词语相似度无关）。

解决方案：
* 可否依靠WordNet的词库来得到词语相似度？
* 或者**使机器自己学会在向量中编码表示相似度**

---
我们想到用词语所处的语境来表示词语。
* **核心思路**：**一个词的意思由频繁出现在它附近的词决定**
* 使用大量含有目标单词的语句来建立目标单词的词向量表示
![0_1530787172058_2b1f014f-f0b4-4e8d-b41d-0ec686fe89ed-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530787172411-2b1f014f-f0b4-4e8d-b41d-0ec686fe89ed-image.png) 

---
## 2. Word2Vec概览
`Word2Vec`(Mikolov et al. 2013)是一个训练词向量的框架。

* 有大量的语料
* 适配的词典中的每一个词都由一个向量表示
* 遍历文本中出现单词`c`的每一个位置，我们假设`c`周围的若干单词为`o`，使用`o`与`c`的词向量相似度来计算通过`o`推出`c`的概率。不断调整词向量以使概率最大化。
![0_1530788239279_c2592089-9346-4638-8f0d-6a0059153cdc-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530788239704-c2592089-9346-4638-8f0d-6a0059153cdc-image.png) 

---
### 目标函数(损失函数)

$$J(\theta)=-\frac1{T}logL(\theta)=-\frac1{T}\sum_{t=1}^T\sum_{-m\le j\le m, j\neq0}logP(w_{t+j}|w_t;\theta)$$

其中，t=1,...,T为语句中的位置，m为周围词的范围，中心词为$w_j$

**调整权重将损失函数值降到最低<=>使预测概率最大化**

---
#### 如何计算P？
对于每个词，我们将使用两个向量：
* $$v_w$$, 当w为目标单词（中心词）
* $$u_w$$, 当w为周围单词

那么，对于一个中心词`c`和周围词`o`，我们可以得到概率公式为：

$$P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in v}exp(u_o^Tv_c)}$$

![0_1530789375609_dd47722d-d6be-48af-b200-629395d416d9-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530789377486-dd47722d-d6be-48af-b200-629395d416d9-image.png) 
![0_1530789414068_b1512cf4-aa55-443a-93aa-facc71bcce42-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530789414483-b1512cf4-aa55-443a-93aa-facc71bcce42-image.png)

* 为什么要使用两个向量？
更容易优化。在最后求两者平均值。

---
#### 两种类型的模型
1. Skip-grams(SG)
使用目标单词作为输入，预测目标单词周围的词，取概率最高的[8]（窗口大小）个词

2. Continuous Bag of Words(CBOW)
使用目标单词周围的[8]个词作为输入，预测目标单词，取概率最高的

---
我们现在已经有一个需要最小化的损失函数了，但是如何使损失函数最小化呢？
### Gradient Descent(梯度下降)
梯度下降是一个最小化损失函数的算法。
梯度下降法，当前θ值，计算其在损失函数中的梯度，再向梯度下降方向变化当前值，直到找到局部最小值。
![0_1530790038690_47ed27fe-967a-46e5-8b2c-3b900304b211-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530790039379-47ed27fe-967a-46e5-8b2c-3b900304b211-image.png) 

### Stochastic Gradient Descent(随机梯度下降)
我们一次对语料库中的所有词语使用梯度下降算法是不现实的，因此我们使用随机梯度下降算法来解决梯度下降的计算资源问题。在随机梯度下降法中，每次我们只取一段sample window，进行梯度下降。