---
layout: post
cover: 'http://bbs.dian.org.cn/assets/uploads/files/1530956820599-f6caaaa7-e4d3-453c-88d8-927dcc21f193-image.png'
title: 'CS224n笔记 - lecture5 - backpropagation'
subtitle: 'backpropagation反向传播'
date: 2018-07-07
categories: CS224n
tags: CS224n 机器学习 深度学习 NLP
---


所有课件及Assignments可见我的[Github:hunto/CS224n](https://github.com/hunto/CS224n)

# Lecture5 - backpropagation （反向传播）

单层神经网络
![0_1530956249043_781bf661-4a85-41ba-9c58-262e4a3850db-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530956249764-781bf661-4a85-41ba-9c58-262e4a3850db-image.png) 

多层神经网络
![0_1530956303304_65c79f5d-d8bb-4f4c-b924-6eff9a077c26-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530956303750-65c79f5d-d8bb-4f4c-b924-6eff9a077c26-image-resized.png) 

**为什么需要更多层网络？**
层数越多，可以表达的问题越复杂

![0_1530956403558_7a2057de-cb9b-40dc-a847-807ee2ec4d63-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530956404164-7a2057de-cb9b-40dc-a847-807ee2ec4d63-image.png) 

## Stochastic Gradient Descent 随机梯度下降

$$\theta^{new} = \theta^{old} - \alpha \nabla_\theta J(\theta)$$

前向传播与反向传播
![0_1530956819974_f6caaaa7-e4d3-453c-88d8-927dcc21f193-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530956820599-f6caaaa7-e4d3-453c-88d8-927dcc21f193-image.png) 

![0_1530956883896_16b2e385-014a-44e5-ab6d-fdac43e34d23-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530956884399-16b2e385-014a-44e5-ab6d-fdac43e34d23-image.png)
