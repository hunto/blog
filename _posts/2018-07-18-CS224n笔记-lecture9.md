---
layout: post
cover: 'http://bbs.dian.org.cn/assets/uploads/files/1531899704111-fcadea37-8490-4960-a8c2-52e4b6363ee9-image.png'
title: 'CS224n笔记 - lecture9 - 梯度消失与更好的RNN'
subtitle: 'Vanishing Gradients and Fancy RNNs(LSTMs and GRUs)'
date: 2018-07-18
categories: 机器学习 NLP CS224n
tags: CS224n 机器学习 深度学习 NLP
---

所有课件及Assignments可见我的[Github:hunto/CS224n](https://github.com/hunto/CS224n)

# Lecture9 - Vanishing Gradients and Fancy RNNs(LSTMs and GRUs)
课件前半部分讲了梯度消失问题，我已在[机器学习梯度消失与梯度爆炸问题详解](https://hunto.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/07/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3.html)中说明，不再赘述。

---

## Gated Recurrent Units
为了改善梯度消失问题，引入了更复杂的隐藏单元，例如GRU。主要思想是
* 保留记忆以捕捉更长的关系，可以决定何时遗忘
* 误差可以根据输入的不同而不同

在标准的RNN中，隐藏层的计算直接基于前一层的输出以及输入$$x$$：

$$h_t=f(W^{(hh)}h_{(t-1)}+W^{(hx)}x_t)$$

而GRU首先会基于当前输入和隐藏状态计算一个更新门（另一层）：

$$z_t=\sigma (W^{(z)}x_t+U^{(z)}h_{t-1})$$

再利用相同的方法不同的权值计算一个reset gate：

$$r_t=\sigma (W^{(r)}x_t+U^{(r)}h_{t-1})$$

然后就可以得到与一般RNN之前不同的记忆内容：

$$\tilde h_t = tanh(Wx_t + r_t\circ Uh_{t-1})$$

如果reset gate的元素为0，则会遗忘之前的记忆，只会存储新输入的信息。

最后得到的隐藏层h_t为：

$$h_t = z_t \circ h_{t-1} + (1-z_t)\circ \tilde h_t$$

从这里我们可以看出，update gate决定的是当前输入是否改变隐藏层状态。如果$$z_t$$为1，则$$h_t$$仅会复制前一层的状态，与当前输入无关。而reset gate则是控制记忆是否被遗忘或遗忘的多少，如果$$r_t$$为0，则之前层的记忆会被遗忘。

用图示的方法更为直观：
![0_1531897658037_ca5b979d-bda7-480e-a4a8-39aa0512f0db-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531897662571-ca5b979d-bda7-480e-a4a8-39aa0512f0db-image.png)

---

## GRU是如何改善梯度消失问题的？
![0_1531898121096_57aa0564-906b-448c-a5db-61b3e0999871-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531898121560-57aa0564-906b-448c-a5db-61b3e0999871-image.png) 

GRU可以让网络剪掉不必要的连接，减少反向传播的深度。

![0_1531898257043_4e443e42-496d-460d-beab-0ca6caa64a27-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531898258203-4e443e42-496d-460d-beab-0ca6caa64a27-image.png) 

![0_1531898303936_7729d00a-962d-4580-9dd8-3b4f9f81e1a2-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531898305069-7729d00a-962d-4580-9dd8-3b4f9f81e1a2-image.png) 


---

## Long-short-term-memories (LSTMs)
LSTM是一个比GRU更复杂的RNN模型，它允许修改每一个时间节点。
* Input gate

    $$i_t = \sigma (W^{(i)}x_t+U^{(i)}h_{t-1})$$

* Forget

    $$f_t=\sigma(W^{(f)}x_t+U^{(f)}h_{t-1})$$

* Output

    $$o_t = \sigma (W^{(o)}x_t+U^{(o)}h_{t-1})$$

* New memory cell

    $$\tilde c_t = tanh(W^{(c)}x_t+U^{(c)}h_{t-1})$$

由此得到的最终memory cell为：

$$c_t = f_t \circ c_{t-1} + i_t \circ \tilde c_t$$

最终的隐藏层状态为：

$$h_t = o_t \circ tanh(c_t)$$


**LSTM模型的可视化如下**

![0_1531899702873_fcadea37-8490-4960-a8c2-52e4b6363ee9-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531899704111-fcadea37-8490-4960-a8c2-52e4b6363ee9-image.png) 
![0_1531899727583_8a5596bb-3d69-4a4e-9e5f-e7a83d023117-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531899727954-8a5596bb-3d69-4a4e-9e5f-e7a83d023117-image.png) 

图片来自[Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)，推荐阅读。

---

## LSTM可以作为所有序列问题的优秀模型
* 非常powerful，特别是网络更深时。
* 对于大量数据很有用

---

## Bidirectional RNNs (双向RNN)
文本分类时，我们同时需要前后文，因此使用双向RNN可以同时存储前后文信息。
![0_1531900511334_b5777b87-f6d1-40f0-b832-21f6b8cee247-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531900513190-b5777b87-f6d1-40f0-b832-21f6b8cee247-image.png) 

**更深层的双向RNN如下：**

![0_1531900574873_1b1a2736-2ca0-410c-8f72-c481ae530eb4-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531900585828-1b1a2736-2ca0-410c-8f72-c481ae530eb4-image.png) 

---

![0_1531900643367_196f9718-db1e-4c4a-8edc-18f69f4e93e8-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531900646983-196f9718-db1e-4c4a-8edc-18f69f4e93e8-image.png) 
