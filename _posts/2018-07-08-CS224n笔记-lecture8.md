---
layout: post
cover: 'https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531033583603-097a4c2a-7f99-4812-b7ef-e20bd343ea5f-image.png'
title: 'CS224n笔记 - lecture8 - Recurrent Neural Networks(RNN)'
subtitle: 'Recurrent Neural Networks(RNN)'
date: 2018-07-08
categories: CS224n
tags: CS224n 机器学习 深度学习 NLP
---

所有课件及Assignments可见我的[Github:hunto/CS224n](https://github.com/hunto/CS224n)

# Lecture8 - Recurrent Neural Networks （循环神经网络）
## Language Modeling （语言模型）
语言模型是一个预测一段文字的下一个词是什么的任务。
![0_1531030485353_b2c16532-829b-4665-8900-283749b33a08-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531030485828-b2c16532-829b-4665-8900-283749b33a08-image.png) 

正式地说，Language Model就是，通过已给的词的序列$${X_1, X_2, ..., X_t}$$，计算出下一个词$$X_{t+1}$$的概率：

$$P(x^{(t+1)}=w_j|x^{(t)}, ..., x^{(1)}) $$

其中，$$W_j$$是词库中的一个词。

---
我们每天都在使用Language Model：

![0_1531030877346_0c0b0750-d273-4744-9c0c-c3e43a09f395-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531030878196-0c0b0750-d273-4744-9c0c-c3e43a09f395-image-resized.png) 
![0_1531030891358_03811854-4db8-4898-b1e8-d03b0a859a66-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531030892184-03811854-4db8-4898-b1e8-d03b0a859a66-image-resized.png) 

---
## n-gram Language Models
我们该如何学习一个语言模型呢？
**使用n-gram Language Model**

一个 n-gram 是一个由连续的词组成的序列：
* **uni**grams: "the", "students", "opened", "their"
* **bi**grams: "the students", "students opened", "opened their"
* **tri**grams: "the students opened", "students opened their"
* **4 -** grams: "the students opened their"

n-gram的思路是统计每个gram出现的次数，用这些来预测下一个词。

---
### 具体实现
我们首先做一个简单的**假设**: 第t+1个词只由它之前的(n-1)个词决定，即：

$$P(x^{(t+1)}|x^{(t)}, ..., x^{(1)})=P(x^{(t+1)}|x^{(t)}, ..., x^{(t-n+2)}) = \frac{P(x^{(t+1)}, x^{(t)}, ..., x^{(t-n+2)})}{P(x^{(t)}, ..., x^{(t-n+2)})}$$

但是，我们怎么得到这n个gram和n-1个gram的概率呢？
**通过在大语料库中计算它们的个数**

$$P(x^{(t+1)}|x^{(t)}, ..., x^{(1)}) \approx \frac {count(x^{(+1)}, x^{(t)}, ..., x^{(t-n+2)})} {count(x^{(t)}, ..., x^{(t-n+2)})}$$

---
### n-gram Language Model Example
假设我们正在学习一个4-gram语言模型。
![0_1531032283628_30064fc6-9ac3-4ff9-bce2-53d3527279ba-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531032284112-30064fc6-9ac3-4ff9-bce2-53d3527279ba-image.png) 
若在语料库中：
* `students opened their`出现了1000次
* `students opened their books`出现了400次 => `P(books|students opened their) = 0.4`
* `students opened their exams`出现了100次 => `P(exams|students opened their) = 0.1`
 
---
### **n-gram模型的问题**
1. 如果语料库中一个词从没有出现在某个词序列(`students opened their`)后，那个词的概率就是0
解决方案：给每个词的count值加一个很小的量 —— smoothing（平滑处理）

2. 如果语料库中压根就没有这个词序列(`students opened their`)，所有词的概率都是0！
解决方案：使用更小的gram(`opened their`)来替代 —— backoff

3. n-gram模型是巨大的，它需要给每个词都与预测序列计算一次概率，空间以及时间复杂度均很高。

那么，我们能不能用神经语言模型(neural Language Model)来实现呢？
当然可以。

可以用一个固定大小的CNN来实现。

---
## A fixed-window neural Language Model
![0_1531033171764_3f5eb738-4161-422b-96ce-78ee7578d4da-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531033172349-3f5eb738-4161-422b-96ce-78ee7578d4da-image.png) 

这样的模型相较于n-gram的提升：
* 没有稀疏的问题
* 模型大小为O(n)，相较于n-gram的O(exp(n))提升巨大

但是也存在问题：
* 固定的window太小
* 增加window大小的同时也会增加W的大小
* window永远不可能足够大
* 输入的4个x都有不同的W，当挪动window时，它们并不共享权值。例如`the students opened their`与`students opened their books`虽然都有`students`，但是不是同一个位置，并不能共享权值。

因此，我们需要一个可以处理任何长度输入的神经模型。

---
## **OH! Recurrent Neural Networks (RNN)**
![0_1531033583135_097a4c2a-7f99-4812-b7ef-e20bd343ea5f-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531033583603-097a4c2a-7f99-4812-b7ef-e20bd343ea5f-image.png) 

![0_1531033696665_a3742532-810a-4c9a-aca7-4f0abcf13d92-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531033697269-a3742532-810a-4c9a-aca7-4f0abcf13d92-image.png) 

---
**Loss Function损失函数**
我们一般使用Cross-Entropy交叉熵作为损失函数。对于时间点t中预测得到的下一个词$$\hat{y^{(t)}}$$，以及真实的下一个词$$y^{(t)} = x^{(t+1)}$$，有该节点的损失函数：

$$J^{(t)}(\theta) = CE(y^{(t)}, \hat{y}^{(t)}) = - \sum_{j=1}^{|V|}y_j^{(t)}log\hat{y}_j^{(t)}$$

总损失函数为：

$$J(\theta)=\frac1T\sum_{t=1}^TJ^{(t)}(\theta)$$

![0_1531034914539_f9a5333c-510c-4d60-8d6e-dad056937bb4-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531034915196-f9a5333c-510c-4d60-8d6e-dad056937bb4-image.png) 

---
**反向传播** 

这里可以参考[CS224n笔记8 RNN和语言模型](http://www.hankcs.com/nlp/cs224n-rnn-and-language-models.html)，写的很好
![0_1531036282246_2c2e756a-37c6-4516-8c3b-b9e82210c090-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531036283333-2c2e756a-37c6-4516-8c3b-b9e82210c090-image.png) 
![0_1531036331820_9f65ad3c-ca44-49c6-912c-9c7a503d02ea-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531036332748-9f65ad3c-ca44-49c6-912c-9c7a503d02ea-image.png) 

---
**循环神经网络的优点**
* 可以处理任意长度的输入
* 对于更长的输入，模型大小并不会增加
* 循环神经网络中某一步的计算可以使用之前计算结果。（受之前计算影响，与之前的有关）

**循环神经网络的缺点**
* 递归计算很慢，无法并行计算。
* 很难从中间的步骤中获取到有用的信息。
* **之后还会介绍更多的缺点**
