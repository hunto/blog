---
layout: post
cover: 'https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531903695382-e175447c-6548-4485-9fbc-052f78cd47b0-image.png'
title: 'CS224n笔记 - lecture10 - 机器翻译、seq2seq模型和注意力机制'
subtitle: 'Machine Translation, Sequence-to-sequence and Attention'
date: 2018-07-19
categories: CS224n
tags: CS224n 机器学习 深度学习 NLP
---

所有课件及Assignments可见我的[Github:hunto/CS224n](https://github.com/hunto/CS224n)

# Lecture10 - Machine Translation, Sequence-to-sequence and Attention
# 机器翻译、seq2seq模型和注意力机制

到这里，课程已经过半了，剩余的课程几乎都是由项目驱动，我们将会学到NLP+DL的研究前沿。课程也会变得更高层：不再有梯度计算、有时我们只会对一些知识做概述。
这次的课程包含NLP深度学习的两项核心技术——seq2seq & Attention

---

## Machine Translation
机器翻译是将一个语言的句子翻译成另一语言的任务。
例如将`I love machine learning.`翻译成`我爱机器学习。`

### **早期机器翻译**
早期的机器翻译更多的是基于语言规则、使用词典来构建。

---

### **1990s-2010s： Statistical Machine Translation**
90年代到2010年左右，机器学习多基于统计。
传统统计机器翻译的核心思路是：从数据中学习一个概率模型，使用贝叶斯找到概率最大的句子：

$$\hat y = argmax_yP(y|x)=argmax_yP(x|y)P(y)$$

其中$$P(x y)$$为翻译模型，指导词和短语翻译，训练数据为双语数据。$$P(y)$$为语言模型，指导如何写出好的翻译句子，训练数据为目标翻译语言。

那么，我们如何学习一个$$P(x y)$$模型呢？
首先，我们需要大量的双语模型。
接着，我们需要的翻译模型其实是

$$P(x,a|y)$$

其中$$a$$用于对齐，因为中英文中语言的顺序是不一样的，例如`Chinese eat with chopsticks.`的中文为`中国人用筷子吃饭`，这两句话的词语并不是按顺序对应的。

---

### 对齐
* 有一些词在另一语言中并没有与它对应的单词
![0_1531902906795_63dc734a-6477-429e-aa24-1593559016f0-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531902907238-63dc734a-6477-429e-aa24-1593559016f0-image.png) 

* 有一些词可以翻译为多个词
![0_1531902949831_3465d240-c405-44cc-954a-48822b3b3170-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531902950147-3465d240-c405-44cc-954a-48822b3b3170-image.png) 

* 也有一些词可以由多个词翻译成
![0_1531903000888_a072845d-99fd-491a-8c45-0dda387435e1-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531903001311-a072845d-99fd-491a-8c45-0dda387435e1-image.png)

* 甚至有多对多的情况出现
![0_1531903033268_b7889433-863d-487f-abbc-c86afcdc51a6-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531903034068-b7889433-863d-487f-abbc-c86afcdc51a6-image.png)  

---

对齐如此复杂，怎么训练一个翻译模型呢？

* 可以举出所有y与x的配对组合然后计算概率吗？ -- 当然不行，计算代价太高

我们可以使用一个启发式搜索算法来逐渐构造翻译模型，忽略概率低的假设。
![0_1531903392517_b11dd4f1-9b2c-4ec9-a39e-2952555e9c50-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531903392985-b11dd4f1-9b2c-4ec9-a39e-2952555e9c50-image.png) 

就算这样也会有非常多的组合。
![0_1531903432405_84ac7323-0373-4ba0-8d6d-9e3dc9b4c256-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531903432837-84ac7323-0373-4ba0-8d6d-9e3dc9b4c256-image.png) 

---
之前的课程中，我们说到用传统机器学习做文本分类很困难，于是出现了深度神经网络。现在在机器翻译中，我们也要喊出同样的一句话：

## Oh! Neural Network!
![0_1531903694239_e175447c-6548-4485-9fbc-052f78cd47b0-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531903695382-e175447c-6548-4485-9fbc-052f78cd47b0-image.png) 

### Neural Machine Translation
NMT是一种只使用一个神经网络进行机器翻译的方法。它的神经网络模型叫做sequence-to-sequence(简写为seq2seq)，包括两个RNN。

![0_1531903873760_5abb931b-a2d4-4767-b40c-604d80e021c5-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531903874848-5abb931b-a2d4-4767-b40c-604d80e021c5-image.png) 

网络由两个RNN组成，`Encoder RNN`产生一个对原句子的编码，`Decoder RNN`通过`Encoder RNN`的输出编码来产生目标句子。

**seq2seq**是一种条件语言模型（**Conditional Language Model**）:
* decoder用于预测翻译句子的下一个词，因此它属于`Language Model`
* 由于预测仍然由输入的原句子决定，因此它是有条件的`Conditional`

![0_1531904424331_36693ff3-15fd-4092-9dca-a46079b253d0-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531904426540-36693ff3-15fd-4092-9dca-a46079b253d0-image.png) 

---

### 训练一个seq2seq模型

![0_1531904501538_6abb98a6-a3ec-4a33-bde5-7cbf0e230038-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531904502721-6abb98a6-a3ec-4a33-bde5-7cbf0e230038-image.png) 

**Greedy Decoding**
下图所示的就是greedy decoding（将每一部中输出的词都放入翻译结果）：
![0_1531904832213_6f009516-b3a2-4370-ae3d-269f2e4467d2-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531904832847-6f009516-b3a2-4370-ae3d-269f2e4467d2-image.png)

但是这样做是有问题的：
* 不能撤销决定。例如有一句法语`les pauvres sont démunis (the poor don’t have any money)`，翻译过程如下：
    * the ___
    * the poor___
    * the poor `are`___

那是否有更好的方式？
=> 使用`beam search`来寻找几个可能的假设并选出最合适的一个。

---

### Beam search decoding
我们的目的是寻找到可能性最大的y：
![0_1531905226147_3cd130ec-3942-401b-bc94-c48fa3e1c651-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531905226539-3cd130ec-3942-401b-bc94-c48fa3e1c651-image.png) 

我们可以尝试所有的可能，但是这样开销太大了。

**Beam search**: 在decoder的每一个时间节点，保持跟踪前k个最可能的翻译部分
* `k`叫做`beam size`(通常在5-10)
* 不保证能找到最佳结果
* 但是要高效得多

![0_1531905480428_a598266f-274c-4058-b20e-5d986ceb0135-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531905481253-a598266f-274c-4058-b20e-5d986ceb0135-image.png) 

---

## Neural Machine Translation的优点
* 有更好的效果：得到的结果更流畅，对上下文的理解更好，对近义短语的理解更好。
* 只需要一个单一神经网络就可以完成端到端翻译。
* 需要更少的人类工程：不需要特征工程，对所有语言翻译可以使用同样的方法。

## NMT的缺点
* 可解释性差：难以debug
* 很难控制NMT的结果：
    * 不能轻易地指定翻译规则
    * 安全问题

---

## NMT是NLP深度学习中最大的成功
* 2014：第一篇seq2seq论文发表
* 2016：Google翻译从SMT切换到NMT

这样的变化，将每年成百上千工程师构建的SMT切换成了屈指可数的工程师在几个月内即可构建完成的NMT。

那么，机器翻译问题解决了吗？
显然没有！

机器翻译仍然有许多问题需要解决：
* 在词典外的词
* 训练和测试数据的领域不同
* 长文本语境的保留
* 数据量少的翻译语料

![0_1531906312173_224be2f4-9c76-4643-b04e-0154a584d92f-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531906313867-224be2f4-9c76-4643-b04e-0154a584d92f-image.png) 
![0_1531906341250_d1c6be99-bce2-4896-907d-97d2a3d719fe-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531906342110-d1c6be99-bce2-4896-907d-97d2a3d719fe-image.png) 
![0_1531906351852_ab5c41bb-54a7-49d3-a75d-329d1e091406-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531906352823-ab5c41bb-54a7-49d3-a75d-329d1e091406-image.png) 

---
2017年，在机器翻译领域有一篇重磅论文，[Attention Is All You Need](https://arxiv.org/abs/1706.03762)
下面介绍机器翻译邻域新的改进方法：注意力机制

# **ATTENTION**

### seq2seq的瓶颈
![0_1531906589720_f4528885-1b80-41ec-8543-431bb17408b8-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531906590853-f4528885-1b80-41ec-8543-431bb17408b8-image.png) 

Encoder需要捕捉原句子中的所有信息 - 信息瓶颈。

---

### Attention
注意力机制提供了对于信息瓶颈的解决方案。它的核心思想是：在decoder的每一个时间节点，只专注于原序列的一个特定部分。

我们先使用图表的形式了解attention：

Attention首先对encoder输出的每一个节点计算一个attention score，再使用softmax得到每个点的概率（Attention distribution），由概率得到注意力输出，与decoder的hidden state结合得到当前输出。以下为decoder每一时间节点的状态图。

![0_1531906945204_00db08c3-263f-48fd-b2b4-83847d10b261-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531906946417-00db08c3-263f-48fd-b2b4-83847d10b261-image.png) 
![0_1531907117174_6683661e-0398-4d76-9edd-cef8406ce3be-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531907118170-6683661e-0398-4d76-9edd-cef8406ce3be-image.png) 
![0_1531907128483_e51448fd-04f2-420b-8d0b-77e5d99a7013-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531907129277-e51448fd-04f2-420b-8d0b-77e5d99a7013-image.png) 
![0_1531907143847_35eaeb4a-eee0-470d-928c-c2de051f9a13-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531907144576-35eaeb4a-eee0-470d-928c-c2de051f9a13-image.png) 
![0_1531907157485_11d1feae-0e7a-4cac-b47d-b3b0a26a932c-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531907158765-11d1feae-0e7a-4cac-b47d-b3b0a26a932c-image.png) 
![0_1531907172171_f251b75d-b383-4811-849e-abad125899d0-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531907173767-f251b75d-b383-4811-849e-abad125899d0-image.png) 

---

下面为公式：

* 隐藏状态 

$$h_1,h_2,...,h_n \in R^h$$

* 在时间节点t，有decoder隐藏状态

$$s_t \in R^N$$

* 在这一节点，有attention scores:

$$e^t = [s_t^Th_1,s_t^Th_2,...,s_t^Th_N] \in R^N$$

* 给attention score套一层softmax得到attention distribution

$$\alpha^t=softmax(e^t)\in R^N$$

* 用$$\alpha^t$$乘以对应的encoder的hidden states来计算输出的$$a_t$$

$$a_t = \sum^N_{i=1}a_i^th_i\in R^h$$

* 最后将$$a_t$$与$$s_t$$ concat到一起

$$[a_t;s_t]\in R^{2h}$$

---

## Attention 的优点
* Attention对NMT的效果有重大提升  

    允许decoder专注于确定的部分很有用
* Attention解决了NMT的瓶颈  

    Attention允许decoder直接观察到原句子，解决了瓶颈
* Attention对缓解梯度消失有帮助  

   提供了到达很远的状态的捷径
* Attention有一定的可解释性  
    * 我们可以通过attention distribution看到decoder正在关注什么
    * 我们通过attention直接得到了对齐信息
    * 这一点很酷，因为我们从没有明确地训练一个对齐系统
    * 网络自己学到了对齐
    ![0_1531911099361_ed2f8f93-e779-4603-af7e-4fd47535d9be-image.png](https://raw.githubusercontent.com/hunto/hunto.github.io/master/assets/img/CS224n/1531911100424-ed2f8f93-e779-4603-af7e-4fd47535d9be-image.png) 

---

## seq2seq用途广泛
seq2seq不只是适用于机器翻译，在很多领域也有很好的效果：
* Summarization (long text -> short text)
* Dialogue (previous utterances -> next utterance)
* Parsing (input text -> output parse as sequence)
* Code generation (natural language -> Python code)

