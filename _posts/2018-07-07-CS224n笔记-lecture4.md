---
layout: post
title: 'CS224n笔记 - lecture4'
subtitle: 'Word Window Classification and Neural Networks'
date: 2018-07-07
categories: 机器学习 NLP CS224n
tags: CS224n 机器学习 深度学习 NLP
---

所有课件及Assignments可见我的[Github:hunto/CS224n](https://github.com/hunto/CS224n)

### -------- **多公式预警** --------
# Lecture4 - Word Window Classification and Neural Networks
传统机器学习分类一般使用Logistic Regression或SVM等模型来划分决策边界，即只改变分类间的分界线，而不改变输入的特征向量。

![0_1530932176555_c8acff4d-ddc4-452d-8cef-3fb682cd52b0-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530932176877-c8acff4d-ddc4-452d-8cef-3fb682cd52b0-image.png) 

## softmax分类函数
$$p(y_j=1|x)=\frac{exp(W_y\cdot x)}{\sum_{c=1}^Cexp(W_c\cdot x)}$$

softmax分类函数取权值矩阵W的一行W_j乘上输入向量，归一化得到概率。

## softmax与cross-entropy error(交叉熵误差)
我们的目标是最大化正确的y的概率。因此，最小化正确分类的概率的负对数。

$$-logp(y|x)=-log(\frac{exp(f_y)}{\sum_{c=1}^Cexp(f_c)})$$

因为分类y为one-hot向量，所以loss函数等价于交叉熵函数：
$$H(\hat{y}, y)=-\sum_{j=1}^{C}y_jlog(\hat{y}_j)=-y_ilog(\hat{y}_i)$$

对N个点来说，总loss为：
$$\sum_{i=1}^N-log(\frac{exp(f_{y_i})}{\sum_{c=1}^Cexp(f_c)})$$

平均后加上正则化项，为：
$$\frac1N\sum_{i=1}^N-log(\frac{exp(f_{y_i})}{\sum_{c=1}^Cexp(f_c)}) + \lambda \sum_k \theta_k^2$$

因此，我们可以得到真正的损失函数为：
$$J(\theta) = \frac1N\sum_{i=1}^N-log(\frac{exp(f_{y_i})}{\sum_{c=1}^Cexp(f_c)}) + \lambda \sum_k \theta_k^2$$

当我们有很多特征时，正则化可以减轻过拟合
![0_1530933944164_2628551e-960c-4623-82e3-fe393f7d3f08-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530933944559-2628551e-960c-4623-82e3-fe393f7d3f08-image.png) 

## 一般机器学习在词向量分类中的问题
对于一般的机器学习，没有很多特征向量，权值矩阵维度低，只需要更新决策边界。但是在深度学习或词向量中，需要同时学习权值矩阵和词向量，特征维度很高，极其容易过拟合。
![0_1530934302014_68c28408-ae03-4810-a1e4-d221542b3ede-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530934302609-68c28408-ae03-4810-a1e4-d221542b3ede-image.png) 

### 重新训练词向量中的陷阱
我们有一个预训练的词向量，结果较泛化。
![0_1530934454404_e42dc7af-dcf0-4d60-aac7-cd538868011d-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530934454834-e42dc7af-dcf0-4d60-aac7-cd538868011d-image.png) 

而当我们想用特定语料重新训练词向量时，极易产生分类错误的情况。
![0_1530934523042_f0b68709-c301-473b-a6e5-5f7fb01d564b-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530934523566-f0b68709-c301-473b-a6e5-5f7fb01d564b-image-resized.png) 

**那么，我们该不该训练我们自己的词向量呢？**
* 如果语料库很小==》不要重新训练词向量
* 如果有非常大的数据集==》训练词向量可能会得到更好的结果

_side note:_
* 词向量矩阵L常被称作lookup table
![0_1530934831888_f91e32c3-0788-48b7-a400-9043ab1772ae-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530934832868-f91e32c3-0788-48b7-a400-9043ab1772ae-image.png) 
* Word vectors = word embeddings = word representations (mostly)

## Window Classification
通过之前的介绍，我们对单个单词的分类已经几乎完成了。我们可以通过上下文给单个单词分类，以消除歧异。
所谓window classification，我们可以将特征向量X想象成一个固定大小窗口，内容为文本的一部分，我们直接将窗口内的词语的向量拼接可得到X。
![0_1530935336226_04a6b1e8-254e-4180-8e9e-a016e4421fd3-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530935336663-04a6b1e8-254e-4180-8e9e-a016e4421fd3-image.png) 

由于我们的特征向量X只是原有X的拓展，损失函数与之前相同。

## **Oh! Neural Network**
Softmax (等价于logistic regression)效果有限，只有线性决策边界，当问题变得复杂时，效果不好。但是神经网络可以学到复杂的多的特性和非线性决策边界。**Neural  Network, YES!**
![0_1530935881445_ff2d6a30-d87c-4d73-9cc9-b1c214548dbb-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530935881916-ff2d6a30-d87c-4d73-9cc9-b1c214548dbb-image-resized.png

## **From logistic regression to neural nets**
神经网络的每一个神经元都是一个二分类逻辑回归单元。
![0_1530944741189_e223c4df-4a17-4979-8abd-80e72b4b58f8-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530944741741-e223c4df-4a17-4979-8abd-80e72b4b58f8-image.png) 
![0_1530944804636_1b7f3492-aef5-4a5b-a062-1c4c33f6e8cd-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530944805095-1b7f3492-aef5-4a5b-a062-1c4c33f6e8cd-image.png) 

神经网络同时运行多个逻辑回归，但我们并不需要知道他们预测什么，只需要丢给下一层网络，由最终损失函数自动决定他们预测什么。多层并行逻辑回归网络就构成了多层神经网络。
![0_1530944955931_0db53667-0c83-4087-88ac-f18559c0fd6c-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530944956550-0db53667-0c83-4087-88ac-f18559c0fd6c-image.png) 
![0_1530944968740_545abe04-cf19-479b-97b0-1d2a5cba1976-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530944969334-545abe04-cf19-479b-97b0-1d2a5cba1976-image.png) 
![0_1530944982029_bc52b130-3615-4081-81fa-902153cee05d-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530944982460-bc52b130-3615-4081-81fa-902153cee05d-image-resized.png) 

**为什么要是非线性系统**
因为多层线性系统叠加还是线性系统。
$$W_1W_2...W_nx=Wx$$

![0_1530945207552_da0c29f5-0ad9-4dac-a911-1fac6bee49ba-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530945208262-da0c29f5-0ad9-4dac-a911-1fac6bee49ba-image.png) 

## Feed-forward Computation 前向传播网络
课件中给了一个简单的网络作为例子：
![0_1530945365404_8c1fb10a-76f9-4529-a404-6f77fc630b6b-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530945366572-8c1fb10a-76f9-4529-a404-6f77fc630b6b-image.png) 

这种红点图经常在论文里看到，大致代表单元数；中间的空格分隔开一组神经元，比如隐藏层单元数为2×4。U是隐藏层到class层的权值矩阵：
![0_1530945474416_c42b3315-ed99-4068-8f2d-62a03919a864-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530945475411-c42b3315-ed99-4068-8f2d-62a03919a864-image-resized.png) 

其中a是激活函数：
$$a=\frac{1}{1+exp(-(W^Tx+b))}$$

## The max-margin loss 间隔最大化损失
怎么设计目标函数呢，记$s_c$代表误分类样本的得分，$s$表示正确分类样本的得分。则朴素的思路是最大化$(s−s_c)$ 或最小化 $(s_c−s)$。但有种方法只计算$s_c>s⇒(s_c−s)>0$时的错误，也就是说我们只要求正确分类的得分高于错误分类的得分即可，并不要求错误分类的得分多么多么小。这得到间隔最大化目标函数：
$$J=max(s_c-s,0)$$

但上述目标函数要求太低，风险太大了，没有留出足够的“缓冲区域”。可以指定该间隔的宽度$(s−s_c<\varDelta)$ ，得到：
$$J=max(\varDelta +s_c-s,0)$$

可以调整其他参数使得该间隔为1：
$$J=max(1+s_c-s,0)$$

在这个分类问题中，这两个得分的计算方式为：
$$s_c=U^Tf(Wx_c+b)$$
$$s=U^Tf(Wx+b)$$
通常通过负采样算法得到负例。

另外，这个目标函数的好处是，随着训练的进行，可以忽略越来越多的实例，而只专注于那些难分类的实例。

## 反向传播训练
U的梯度：
![0_1530946339265_59a29cc4-6d5b-4875-923d-5ba3bcb0f767-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530946339659-59a29cc4-6d5b-4875-923d-5ba3bcb0f767-image.png) 

对于偏置的偏导数$\delta_i^{(k)}$:
![0_1530946508230_52bd82c2-794b-49d5-adfd-0f02484e0812-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530946508816-52bd82c2-794b-49d5-adfd-0f02484e0812-image.png) 

最后一片拼图是对词向量的偏导数，由于连接时每个输入单元连到了多个隐藏单元，所以对某个输入单元的偏导数是求和的形式（残差来自相连的隐藏单元）：
![0_1530946544351_e1c3de57-9515-44ad-ada1-af3dd54d60d4-image.png](http://bbs.dian.org.cn/assets/uploads/files/1530946544787-e1c3de57-9515-44ad-ada1-af3dd54d60d4-image.png) 

其中，$W^T \cdot j$是第$j$列，转置后得到行向量；红色部分是误差，相乘后得到一个标量，代表词向量第$j$维的导数。那么对整个词向量的偏导数就是：
$$\frac{\partial s}{\partial x} = W^T\delta$$
