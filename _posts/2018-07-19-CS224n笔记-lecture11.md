---
layout: post
cover: 'https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531969082381-ed374125-8743-439b-847e-4116f2d92987-image.png'
title: 'CS224n笔记 - lecture11 - Attention的更多应用、处理MT大词库的技巧'
subtitle: 'Paying attention to attention and Tips and Tricks for large MT'
date: 2018-07-19
categories: CS224n
tags: CS224n 机器学习 深度学习 NLP
---

所有课件及Assignments可见我的[Github:hunto/CS224n](https://github.com/hunto/CS224n)

# Lecture11 - Paying attention to attention and Tips and Tricks for large MT

Attention是一项通用的深度学习技术，它不仅应用于seq2seq中，今天我们还将介绍attention的更多应用。

## Attention
Attention是一个根据输入向量序列计算元素的权值的技术。

## 几种Attention的变体
* 上一课中提到的attention，我们有encoder 隐藏状态

    $$h_1,...,h_n\in R^{d_1}$$

    有decoder 隐藏状态query

    $$s\in R^{d_2}$$

* Attention总是从attention scores(e)中计算得到attention output `a`，例如：

    $$\alpha = softmax(e) \in R^N$$

    $$a = \sum^N_{i=1}\alpha_ih_i \in R^{d_1}$$


* 但其实，我们有很多种方式计算attention scores(e)

---

### 计算e的几种方式
通过encoder与encoder的hidden states($$h \in R^{d_1}$$, $$s \in R^{d_2}$$)，我们有以下几种方式计算attention scores：
* 基本的点乘attention(Basic dot-product attention)
    
    $$e_i = s^Th_i\in R$$

    * 注意这里e和s的向量大小要是相同的
    * 我们上节课说的attention就是这种计算方式

* 中间乘上权值矩阵(Multiplicative attention)
    
    $$e_i = s^TWh_i \in R$$

    * $$W \in R^{d_2 \times R^{d_1}}$$为权重矩阵

* h和s分别乘不同的矩阵后相加(Addictive attention)

    $$e_i = v^Ttanh(W_1h_i+W_2s)$$

    * 其中，$$W_1 \in R^{d_3 \times d_1}$$, $$W_2 \in R^{d_3 \times d_2}$$为权重矩阵，$$v \in R^{d_3}$$为权重向量

更多信息，可以查看[Deep Learning for NLP Best Practices](http://ruder.io/deep-learning-nlp-best-practices/index.html#attention)

---

## Attention的应用：Pointing to words for language modeling
思路：softmax与pointer混合

[Pointer Sentinel Mixture Models](https://arxiv.org/abs/1609.07843)
![0_1531969078357_ed374125-8743-439b-847e-4116f2d92987-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531969082381-ed374125-8743-439b-847e-4116f2d92987-image.png) 

### Pointer-Sentinel Model
![0_1531969480755_579e91ae-52a0-449f-8b7b-7d3948651f49-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531969482292-579e91ae-52a0-449f-8b7b-7d3948651f49-image.png) 
![0_1531969598076_9333d84e-15a7-4491-b058-3897a7021de1-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531969598615-9333d84e-15a7-4491-b058-3897a7021de1-image.png) 

---

## Attention应用：Intra-Decoder attention for Summarization
[A Deep Reinforced Model for Abstractive Summarization](https://arxiv.org/abs/1705.04304)
![0_1531969822013_97f8f7a1-cc2c-4e16-b872-29038f055e0e-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531969826270-97f8f7a1-cc2c-4e16-b872-29038f055e0e-image.png) 

---

## Attention应用：Similar Seq2Seq Idea as in Translation
![0_1531969984139_64e699bd-f0a7-4e54-ab82-b53f41c3407d-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531969985743-64e699bd-f0a7-4e54-ab82-b53f41c3407d-image.png) 

---

还有很多attention的应用，这里不一一列举了。现在，我们先介绍一些能够提升机器翻译效果的tips & tricks。

## 将NMT拓展到更多语言
* 光是复制原理还不够
    * Transliteration: Christopher ↦ Kryštof
    * Multi-word alignment: Solar system ↦ Sonnensystem

* 需要准备一个大词库
    * 丰富的语法
        * `nejneobhospodařovávatelnějšímu` - Czech = “to the worst farmable one”
        * `Donaudampfschiffahrtsgesellschaftskapitän` – German = Danube steamship company captain
    * 非正式的拼写: `goooooood morning !!!!!`

---

## 解决大词表问题
大词表加大了softmax的计算难度
![0_1531970591328_5769e591-9084-4a96-9ac0-ac5b17a292a0-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531970596138-5769e591-9084-4a96-9ac0-ac5b17a292a0-image.png) 
 
在早期的MT中，会使用较小的词表，但这样并不能说解决了问题：
![0_1531970719921_d193bdad-dcc9-48df-8cc5-6d87051c95c3-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531970725205-d193bdad-dcc9-48df-8cc5-6d87051c95c3-image.png) 

另一种想法是缩小softmax大小，但这样对gpu不友好
![0_1531970814765_cfe20eb7-fbdf-4eff-9eae-27c776e90b33-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531970817666-cfe20eb7-fbdf-4eff-9eae-27c776e90b33-image.png) 

---

### Large-vocab NMT
这个方法的思路是，首先在大词表训练集的子集下训练模型，再在测试时添加一些技巧处理那些出现很少的词。

**Training**
每次都训练一个比原词库小很多的子集，训练多个子集
![0_1531971098389_5829a8dc-34e8-4a22-ba16-a692ff34a0fa-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531971103402-5829a8dc-34e8-4a22-ba16-a692ff34a0fa-image.png) 

例如，每次选取子集大小为5：
![0_1531971191735_0a09c625-b721-4704-80ad-692f2be8e642-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531971194645-0a09c625-b721-4704-80ad-692f2be8e642-image.png) 
![0_1531971217240_da92c5e5-7488-4495-9840-f4dcb47141cb-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531971218386-da92c5e5-7488-4495-9840-f4dcb47141cb-image.png) 
![0_1531971225528_6a0bf7c8-51a9-4f29-afc0-7189520b3018-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531971226359-6a0bf7c8-51a9-4f29-afc0-7189520b3018-image.png) 

**Test**

* 取k个出现频率最高的词 - unigram prob
    `de, , la . et des les …`
* 每个词取`k'`个翻译候选单词
    ![0_1531971395512_2392f140-ab51-47b9-80a4-f135fe9ca049-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531971397273-2392f140-ab51-47b9-80a4-f135fe9ca049-image.png) 

![0_1531971502392_14d48027-e71e-47ca-99cf-b92ac9327078-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531971503627-14d48027-e71e-47ca-99cf-b92ac9327078-image.png) 

---

之后还说到了很多解决大词库的思路
### Byte Pair Encoding
这种方法试图用分词的思想去找出所有有意义的“词素”，其统计方法说来也简单，就是在词频词表里统计所有的ngram组合作为新的更长的ngram：
![0_1531971726609_852ea8ad-544b-4c2d-8a8a-716651065d17-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531971729600-852ea8ad-544b-4c2d-8a8a-716651065d17-image.png)

---

### 字符级别的LSTM(character-based LSTM) 
![0_1531971812894_084f1303-8c0d-479c-b73b-995c01f7c6eb-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531971816398-084f1303-8c0d-479c-b73b-995c01f7c6eb-image.png) 

---

### Hybrid NMT
另有一些混合动力的NMT，大部分情况下在词语级别做翻译，只在需要的时候从字符级去翻译。这个系统的主体是词语级别的LSTM，先在词语级别上做常规的柱搜索，当出现unknown词时，切换到char级别做柱搜索：
![0_1531971894974_b398431f-c24f-4d03-942e-0210a5760f84-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531971897216-b398431f-c24f-4d03-942e-0210a5760f84-image.png) 

**BLEU得分**
![0_1531972018666_e8ff3495-c04e-4fb4-b41d-a2a7187330c4-image.png](https://raw.githubusercontent.com/hunto/blog/master/assets/img/CS224n/1531972022597-e8ff3495-c04e-4fb4-b41d-a2a7187330c4-image.png) 
