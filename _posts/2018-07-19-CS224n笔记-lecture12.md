---
layout: post
cover: 'http://bbs.dian.org.cn/assets/uploads/files/1531978737341-05dd8915-607d-476b-b0a5-20fa417c1cfb-image.png'
title: 'CS224n笔记 - lecture12 - All Attention & TextCNN'
subtitle: 'Transformer Networks and Convolutional Neural Networks'
date: 2018-07-19
categories: 机器学习 NLP CS224n
tags: CS224n 机器学习 深度学习 NLP
---

所有课件及Assignments可见我的[Github:hunto/CS224n](https://github.com/hunto/CS224n)

# Lecture12 - Transformer Networks and Convolutional Neural Networks

## Problems with RNNs = Motivation for Transformers
* 连续计算阻止了平行化
![0_1531978477143_6cc314db-c754-45bc-8d33-29d728231ad1-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531978480722-6cc314db-c754-45bc-8d33-29d728231ad1-image.png) 
* 尽管有GRU和LSTM，RNN仍然需要attention机制来处理长距离的关系 - 状态间需要关系计算的长度随序列的增加而增加
* **但，如果attention能使我们得到任意状态，那么其实我们并不需要RNN？**

---

## Transformer概述
* Sequence-to-sequence
* Encoder-Decoder
* Task: machine translation with parallel corpus(平行语料)
* Predict each translated word
* Final cost function is standard cross-entropy loss on top of a softmax classifier

![0_1531978728127_05dd8915-607d-476b-b0a5-20fa417c1cfb-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531978737341-05dd8915-607d-476b-b0a5-20fa417c1cfb-image.png) 

---

在这里不得不说这篇非常有名的论文，[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)，Transformer也是出自这篇论文。

## Transformer Basics
我们首先定义一个transformer的基础部分：attention layers

### Dot-Product Attention
这里我们使用的是Dot-Product Attention。
* 输入：query q和指向输出output的key-value对的集合
* query, keys, values, output都是向量
* output是values的权重计算结果，其中
    * 每一个value的权重都由一个内部产生的query以及相应的key计算出
    * queries和keys都有相同的维度$$d_k$$, value的维度为$$d_v$$

$$A(q, K, V) = \sum_i\frac {e^{q\cdot k_i}}{\sum_je^{q\cdot k_j}}v_i$$

当我们有query的集合Q，Q就是一个矩阵，上式变成：

$$A(Q,K,V) = softmax(QK^T)V$$

![0_1531981013480_49e52467-686b-47d2-9bcf-c0086b922f50-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531981015009-49e52467-686b-47d2-9bcf-c0086b922f50-image.png) 

---

### Scaled Dot-Product Attention
这里出现了一个问题，当$$d_k$$特别大时，$$q^Tk$$的变化就会很大，softmax中的一些值就会变得很大，softmax也就会变得很大，因此它的梯度会变小。

**解决方法**
里面除以query/key向量的长度的平方根来缩小softmax内值的大小。

$$A(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

![0_1531981353252_360e50ee-4e0e-4ae0-a765-c9922b780b8f-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531981355196-360e50ee-4e0e-4ae0-a765-c9922b780b8f-image.png) 

---

### Self-attention and Multi-head attention
* 输入的词向量可以是queries,keys,values
* 换言之，词向量间互相选择
* Word vector stack = Q = K = V

**问题**：只有一种方式能使词之间互相影响
**解决方式**：Multi-head attention
* 首先将Q, K, V放入h个W矩阵的空间中
* 再应用attention，将attention的输出concat到一起，然后经过一个线性层。

![0_1531982281168_8eb71577-ccb9-4ada-a5d0-f2aa5e7afaeb-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531982282474-8eb71577-ccb9-4ada-a5d0-f2aa5e7afaeb-image.png) 

![0_1531982335988_7f1f920d-d427-4317-be59-3047b27a1338-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531982336580-7f1f920d-d427-4317-be59-3047b27a1338-image.png) 

---

### Complete transformer block
* 每一个block有两个"sublayers"
    1. Multihead attention
    2. 2 layer feed-forward Nnet (with relu)

![0_1531982703822_1140e30f-f126-4fe2-8c3a-38aca3441988-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531982705209-1140e30f-f126-4fe2-8c3a-38aca3441988-image.png) 

这两层之间还包括：
* Residual(short-circuit) connection and LayerNorm
    * LayerNorm(x + Sublayer(x))
    * Layernorm改变输入，将输出的方差为1,均值为0

![0_1531983029908_91da9883-4131-4021-a265-e93ec44dc1d3-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531983030686-91da9883-4131-4021-a265-e93ec44dc1d3-image.png) 
这部分Layer Normalization可见这篇论文：
[Layer Normalization by Ba, Kiros and Hinton](https://arxiv.org/pdf/1607.06450.pdf)

---

### Encoder Input
* 词的表示使用的是byte-pair encodings（见上一篇lecture）
![0_1531983249082_c4c62feb-9b45-4731-be99-d2742a8cb6f2-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531983250105-c4c62feb-9b45-4731-be99-d2742a8cb6f2-image.png) 
* 同时也加入了positional encoding，这样同样的词在不同的位置会有不同的表示。
![0_1531983357748_ede1e158-248a-4702-8c75-b22c6d26ff2e-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531983358889-ede1e158-248a-4702-8c75-b22c6d26ff2e-image.png) 

---

### Complete Encoder
* 在Encoder的每一块中，我们使用了与前一层相同的Q,K,V
* blocks重复6次
![0_1531983562579_22cb5672-4ea3-4aad-9798-5ea5c832f795-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531983563750-22cb5672-4ea3-4aad-9798-5ea5c832f795-image.png) 

---

### layer5的attention可视化
![0_1531984015977_8bc1274f-9225-46f0-a94c-f15a5a84f97a-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531984017092-8bc1274f-9225-46f0-a94c-f15a5a84f97a-image-resized.png) 
可以看到，词开始注意到了其它词。

### attention可视化: 隐藏的指代分辨
![0_1531984119578_27cd9bea-a7f4-4fbf-8eb6-ff170aff6f03-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531984120673-27cd9bea-a7f4-4fbf-8eb6-ff170aff6f03-image.png) 
在layer5中，`it's`对它所指代的词`law`的attention已经很大了。

---

### Transformer Decoder
![0_1531978728127_05dd8915-607d-476b-b0a5-20fa417c1cfb-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531978737341-05dd8915-607d-476b-b0a5-20fa417c1cfb-image.png) 
* decoder中不再是2个sublayer
* masked decoder self-attention on previously generated outputs:
![0_1531984978584_b9ba5342-b561-4bb7-b470-445a7000e484-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531984979604-b9ba5342-b561-4bb7-b470-445a7000e484-image.png) 
* Encoder-Decoder的attention，query来自前一层decoder layer，keys和values来自encoder的输出
![0_1531985086237_0e126a9d-ed4e-452c-8a53-69b7c18df43c-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531985087328-0e126a9d-ed4e-452c-8a53-69b7c18df43c-image.png) 
* blocks也会重复6次

---

### Tips and tricks of the Transformer
在All Attention论文中提到的一些特性：
* Byte-pair encodings
* Checkpoint averaging
* ADAM optimizer with learning rate changes
* Dropout during training at every layer just before adding residual
* Label smoothing
* Auto-regressive decoding with beam search and length penalties
* 总之，模型很难部署，同时不像LSTM一样，all attention很难在其他block结构中表现好

---

## **CNN**
* CNN的主要思想：如果我们为语料中的每种可能的短语计算多种向量会怎样？
* 不管怎样，短语是符合语法规则的，CNN得到的短语也是符合的
* 示例：`the country of my birth`可以计算的向量有：
    `the country`, `country of`, `of my`, `my birth`, `the country of`, `country of my`, `of my birth`, `the country of my`, `country of my birth`
* 然后把这些向量分组
* 这样做从语言学上来说不是很可信，但是计算非常快

### 什么是卷积
这里就不做介绍了
![0_1531986015212_475231f2-25ca-40d6-8e2a-5c12b4264a04-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531986017581-475231f2-25ca-40d6-8e2a-5c12b4264a04-image.png) 

### 单层CNN - 卷积层
* 使用了一层卷积层和池化层的简单结构
* 基于"Convolutional Neural Networks for Sentence Classification" (TextCNN)
* Word vectors

    $$x_i \in R^k$$

* Sentence(就是直接把词向量concat到一起)

    $$x_{1:n}=x_1\oplus x_2 \oplus ... \oplus x_n$$

* Concatenation of words in range: 

    $$x_{i:i+j}$$
    
    就是把j个词的向量加在一起做卷积

* Convolutional filter:

    $$w \in R^{hk}$$

    这里用一个大小为h的window来扫过句子

* 卷积核可以是2或者更高，比如3：
    ![0_1531986605446_628e9077-83f4-4f96-a06e-cdd0395fe1c2-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531986605900-628e9077-83f4-4f96-a06e-cdd0395fe1c2-image.png) 

* 计算得到卷积层的值

    $$c_i = f(W^Tx_{i:i+h-1}+b)$$

* 把多个c放到一个数组中，构成一个feature map

    $$c = [c_1, c_2,...,c_{n-h+1}] \in R^{n-h+1}$$

    ![0_1531987240258_cfed3270-b0a4-4ca4-bdf6-3a30abe9227c-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531987240598-cfed3270-b0a4-4ca4-bdf6-3a30abe9227c-image.png) 

---

### 池化层
* max-over-time pooling layer
* 思想：从feature map

    $$c=[c_1,c_2,...,c_{n-h+1}]\in R^{n-h+1}$$

捕捉最重要的特性

* 池化单个数：
    
    $$\hat c = max\lbrace c\rbrace$$

这样就可以得到卷积层提取出的特征了，但我们希望得到更多的特征，怎么办？

---

### Multiple filters

* 使用多个filter size来构建多个大小的窗口
* 因为用的是max-pooling，c的长度不受限制，因此我们可以使用多个不同的size，就像n-gram一样。

---

### Multi-channel(多通道) 思想
TextCNN有几种：
* CNN-rand: 句子中的的word vector都是随机初始化的，同时当做CNN训练过程中需要优化的参数； 
* CNN-static: 句子中的word vector是使用word2vec预先对语料库进行训练好的词向量表中的词向量。且在CNN训练过程中作为固定的输入，不作为优化的参数; 
* CNN-non-static: 句子中的word vector是使用word2vec预先对语料库进行训练好的词向量表中的词向量。在CNN训练过程中作为固定的输入，做为CNN训练过程中需要优化的参数； 

这里说的是用word2vec训练的static TextCNN
* 使用预训练的词向量进行初始化(Word2Vec或Glove)
* 使用两个CNN copies
* 只反向传播一个CNN，保持另外一个为'static'
* 两个通道的结果都加到c_i中，再进行max-pooling

---

### CNN层之后进行分类
多个filter得到的max-pooling结果构成一个向量：

$$z = [\hat c_1,...,\hat c_m]$$

最后再加上softmax层得到归一化概率：

$$y = softmax(W^{(s)}z+b)$$     

---

### TextCNN
![0_1531988312389_127b1e94-6c76-4208-a373-1adb3024183c-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531988312880-127b1e94-6c76-4208-a373-1adb3024183c-image.png) 

这是Kim在2014年论文中提到的TextCNN结构。

---

### 让模型效果更好的tricks - Dropout
* 思想：随机将隐藏层中的部分特征移除
* 随机构建一个伯努利矩阵r。r可以控制哪些特征要被drop
* 在训练的时候删除部分特征

    $$y = softmax(W^{(s)}(r\circ z)+b)$$

* 因此在训练的时候，只会通过r中等于1的部分影响梯度进行反向传播

* 为什么要Dropout？
    可以防止过拟合
* 来源：Paper: Hinton et al. 2012: Improving neural networks by preventing co-adaptation of feature detectors

---

### 另一个trick - regularization
* 使用l2正则来正则化每个分类的权重向量，使其适配一个超参数s
* 如果$$W_c$$的l2正则大于s, 将其缩小至s
* 这种方法不是很常见

---

### kim的TextCNN中的一些超参数
* 非线性激活函数: Relu
* Window Filter sizes h = 3, 4, 5
* 每个filter size有100个feature map
* Dropout p = 0.5
* L2正则的softmax约束超参数s = 3
* SGD training的batch size为50
* 词向量：预训练的word2vec，维度为300

---

![0_1531990389588_8f1e53c6-2d95-45ee-84a8-e46c9c46c9a7-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531990390308-8f1e53c6-2d95-45ee-84a8-e46c9c46c9a7-image.png) 

---

![0_1531990412321_94050c5d-b0b6-4b9c-b36f-6c877f3c26a9-image.png](http://bbs.dian.org.cn/assets/uploads/files/1531990412653-94050c5d-b0b6-4b9c-b36f-6c877f3c26a9-image.png) 
